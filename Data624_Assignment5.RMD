---
title: "Data624 Assignment5"
author: "Ritesh Lohiya"
date: "March 8, 2019"
output: html_document
---

#Data624 Assignment5

#Chapter 7: 

```{r}
suppressMessages(suppressWarnings(library(fpp2)))
suppressMessages(suppressWarnings(library(readxl)))
suppressMessages(suppressWarnings(library(seasonal)))
```

#### 7.1 Consider the pigs series - the number of pigs slaughtered in Victoria each month.

#### a.  Use the ses function in R to find the optimal values of alpha and l0, and generate forecasts for the next four 

```{r}
str(pigs)
head(pigs)
```

```{r}
ses_pigs <- ses(pigs, h = 4)
```

```{r}
#model
ses_pigs$model
```

####b. Compute a 95% prediction interval for the first forecast using y = +-1.96s where s is the standard deviation of the residuals. Compare your interval with the interval produced by R. 

```{r}
ses_pigs$upper[1, "95%"]
ses_pigs$lower[1, "95%"]
```

```{r}
#using formula
s <- sd(ses_pigs$residuals)
ses_pigs$mean[1] + 1.96*s
ses_pigs$mean[1] - 1.96*s
```

#### We can see that there is difference in the value in the above 2 cases.

```{r}
#plot
autoplot(ses_pigs) +  autolayer(ses_pigs$fitted)
```

#### 7.5 Data set books contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days' sales for paperback and hardcover books.

#### a. Plot the series and discuss the main features of the data.

```{r}
str(books)
head(books)
```

```{r}
#plot
autoplot(books)
```

#### The sales of books increased with time and thete is lot of variations with out any particular patern.

#### b. Use the ses() function to forecast each series, and plot the forecasts.

```{r}
ses_paperback <- ses(books[, "Paperback"], h = 4)
ses_hardcover <- ses(books[, "Hardcover"], h = 4)
```

```{r}
autoplot(books[, "Paperback"], series = "Paperback") +   autolayer(ses_paperback, series = "Paperback") + 
autolayer(books[, "Hardcover"], series = "Hardcover") + autolayer(ses_hardcover, series = "Hardcover", PI = FALSE) +   ylab("Sales") +
ggtitle("paperback and hardcover sales")
```

#### We can see the forecast is really flat.

#### c. Compute the RMSE values for the training data in each case.

```{r}
sqrt(mean(ses_paperback$residuals^2))
sqrt(mean(ses_hardcover$residuals^2))
```

#### The residuals of hardcover sales are smaller than the one of paperback sales accprding to the RMSE values

#### 7.6.a. Now apply Holt's linear method to the paperback and hardback series and compute four-day forecasts in each case.

```{r}
holt_paperback <- holt(books[, "Paperback"], h = 4)
holt_hardcover <- holt(books[, "Hardcover"], h = 4)
```

```{r}
#plot
autoplot(books[, "Paperback"]) +  autolayer(holt_paperback)

autoplot(books[, "Hardcover"]) +  autolayer(holt_hardcover)
```

#### There is linear trend in the forecasts.

#### b. Compare the RMSE measures of Holt's method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt's method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.

```{r}
s_paperback <- sqrt(mean(holt_paperback$residuals^2))
s_hardcover <- sqrt(mean(holt_hardcover$residuals^2))
s_paperback
s_hardcover
```


####RMSE values are now lower than the Holt's method. If there isn't any particular trend in data, it would be better to use SES method

#### c. Compare the forecasts for the two series using both methods. Which do you think is best?

#### Hardcover sales forcast are better than the paperback sales forcast.

#### d. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using ses and holt.

```{r}

holt_paperback$upper[1, "95%"]
holt_paperback$lower[1, "95%"]
```

```{r}
holt_paperback$mean[1] + 1.96*s_paperback
holt_paperback$mean[1] - 1.96*s_paperback
```

```{r}
holt_hardcover$upper[1, "95%"]
holt_hardcover$lower[1, "95%"]
```

```{r}
holt_hardcover$mean[1] + 1.96*s_hardcover
holt_hardcover$mean[1] - 1.96*s_hardcover
```

#### The prediction interval for the first forecast for each series are almost same for all methods. It is different from the ses case, in which the PI was different when it was calculated by ses function and formula respectively.

####7.7 For this exercise use data set eggs, the price of a dozen eggs in the United States from 1900-1993. Experiment with the various options in the holt() function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.

####[Hint: use h=100 when calling holt() so you can clearly see the differences between the various options when plotting the forecasts.]

####Which model gives the best RMSE?

```{r}
str(eggs)
head(eggs)
autoplot(eggs)
```

#### We can see downward trend for the price of eggs here. Using holt function will be good here.

#### First, just use holt function without using any options.

```{r}
holt_eggs <- holt(eggs, h = 100)
autoplot(holt_eggs) +   autolayer(holt_eggs$fitted)
```

####he predicted price is going to be below 0.



#### Use holt function with damped option.

```{r}
holt_damped_eggs <- holt(eggs, damped = TRUE, h = 100)
autoplot(holt_damped_eggs) +   autolayer(holt_damped_eggs$fitted)
```

#### The point forecasts didn't reflect the existing trend.

#### Use holt function with Box-Cox transformation.

```{r}
holt_BoxCox_eggs <- holt(eggs, lambda = BoxCox.lambda(eggs), h = 100)
autoplot(holt_BoxCox_eggs) +   autolayer(holt_BoxCox_eggs$fitted)
```

#### Now it reflects the existing trend.

#### Use holt function with Box-Cox transformation and damped option.

```{r}
holt_BoxCox_damped_eggs <- holt(eggs,damped = TRUE, lambda = BoxCox.lambda(eggs), h = 100)
autoplot(holt_BoxCox_damped_eggs) +   autolayer(holt_BoxCox_damped_eggs$fitted)
```

#### These are decreasing trend and does not look good.

#### show RMSE values for each model

```{r}
sqrt(mean(holt_eggs$residuals^2))
```

```{r}
sqrt(mean(holt_damped_eggs$residuals^2))
```

```{r}
sqrt(mean(holt_BoxCox_eggs$residuals^2))
```
 
```{r}
sqrt(mean(holt_BoxCox_damped_eggs$residuals^2))
```

#### BoxCox transformation improves accuracy of the model. Holt's method with damped option just prohibits the forecasts to be below 0.


#### The best model was the Box-Cox transformation with Holt's linear method. 


####7.8 Recall your retail time series data (from Exercise 3 in Section 2.10).



```{r}
retaildata <- readxl::read_excel("C:/Users/rites/Documents/GitHub/Data624_Assignment1/retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349873A"],frequency=12, start=c(1982,4))
autoplot(myts) + xlab("Time") + ylab("Sales")
```


#### a. Why is multiplicative seasonality necessary for this series?

```{r}
autoplot(myts)
```

#### Seasonality indices increased when the retail sales increased. Multiplicative seasonality can reflect the situation in the model, while additive seasonality can't.



#### b. Apply Holt-Winters' multiplicative method to the data. Experiment with making the trend damped.

```{r}
ets_AAM_retail <- hw(myts, seasonal = "multiplicative")
ets_AAdM_retail <- hw(myts,seasonal = "multiplicative",damped = TRUE)
autoplot(ets_AAM_retail)
autoplot(ets_AAdM_retail)
```

#### When damped option was used the forcast was slow.

#### c. Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?

```{r}
error_ets_AAM_retail <- tsCV(  myts, hw, h = 1, seasonal = "multiplicative")
error_ets_AAdM_retail <- tsCV( myts, hw, h = 1, seasonal = "multiplicative", damped = TRUE)
sqrt(mean(error_ets_AAM_retail^2, na.rm = TRUE))
sqrt(mean(error_ets_AAdM_retail^2, na.rm = TRUE))
```

####RMSE are lmost same. So damped model will be good because it will prohibit the limitless increase of sales forecast.

#### d. Check that the residuals from the best method look like white noise.

```{r}
checkresiduals(ets_AAdM_retail)
```

#### The residuals don't look like white noise. 

#### e. Now find the test set RMSE, while training the model to the end of 2010. Can you beat the seasonal naïve approach from Exercise 7 in Section 3.7?

```{r}
myts_train <- window(myts, end = c(2010, 12))
myts_test <- window(myts, start = 2011)
```

#### Holt-Winters' method with damped option.

```{r}
ets_AAdM_retail_train <- hw(myts_train, h = 36, seasonal = "multiplicative", damped = TRUE)
autoplot(ets_AAdM_retail_train)
accuracy(ets_AAdM_retail_train, myts_test)
```

#### Holt-Winters' method.

```{r}
ets_AAM_retail_train <- hw(myts_train, h = 36, seasonal = "multiplicative")
autoplot(ets_AAM_retail_train)
accuracy(ets_AAM_retail_train, myts_test)
```

#### This has better accuracy than using the option. 


#### 7.9 For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?

```{r}

fc_stl_ets_retail_train <- myts_train %>% stlm( s.window = 13, robust = TRUE, method = "ets", lambda = BoxCox.lambda(myts_train) ) %>%
forecast( h = 36, lambda = BoxCox.lambda(myts_train) )

autoplot(fc_stl_ets_retail_train)
accuracy(fc_stl_ets_retail_train, myts_test)

# try forecasting without doing transformation.

fc_stl_ets_retail_train_without_tr <- myts_train %>% stlm( s.window = 13, robust = TRUE,  method = "ets" ) %>% forecast(h = 36)

autoplot(fc_stl_ets_retail_train_without_tr)
accuracy(fc_stl_ets_retail_train_without_tr,  myts_test)

```


#### ETS forecasting after STL decomposition 'without' Box-Cox transformation yielded better result than when ETS(A, Ad, M) or ETS(A, A, M).














